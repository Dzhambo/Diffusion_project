{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Задачи данного проекты:\n",
    "\n",
    "- выбрать хотя бы два разных датасета (они могут быть и из компьютерного зрения, так и нет)\n",
    "- выбрать хотя бы две метрики качества (одну внутреннюю NLL, а другую внешнюю - например, качество распознавания by 3rd-party classifier)\n",
    "- пообучать генераторы с разным количеством шагов (хотя бы 3-5 значений)\n",
    "- сгенерировать примеры на тесте - и здесь так же интересно - что если мы остановим генерировать раньше , чем последний шаг по времени, который был использован для обучения (т.е. исследование early stopping on inference)\n",
    "- ну и посмотреть на подтипы guidance (например, задавать класс объектов) с учетом сказанного выше + интересно, как влияют разные стратегии управления дисперсией при генерации (процедуры эволюции альфа)\n",
    "\n",
    "В итоге нужно будет предоставить код на своем гитхабе + детальный отчет о проделанной работе (в виде презентации), которую можно выложить там же."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ssl\n",
    "ssl._create_default_https_context = ssl._create_unverified_context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from torchvision.transforms import Compose, ToTensor, Lambda\n",
    "from torchvision.datasets.mnist import MNIST, FashionMNIST\n",
    "from torchvision.datasets import CIFAR10, StanfordCars\n",
    "\n",
    "SEED = 0\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read FasionMNIST Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Diffusion_project.utils import show_first_batch, transform_data_for_show\n",
    "\n",
    "dataset = transform_data_for_show(CIFAR10, batch_size=32)\n",
    "show_first_batch(dataset)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Some visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Diffusion_project.models import CustomDiffusionModel\n",
    "from Diffusion_project.unet import CustomUnet\n",
    "\n",
    "n_steps, min_beta, max_beta = 10, 10 ** -4, 0.02\n",
    "ddpm = CustomDiffusionModel(CustomUnet(), n_steps=n_steps, min_beta=min_beta, max_beta=max_beta, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Diffusion_project.utils import show_forward\n",
    "\n",
    "show_forward(ddpm, dataset, device)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def training(ddpm, dataloader, n_epochs, optimizer, device, store_path='ddpm.pt'):\n",
    "    loss_function = torch.nn.MSELoss()\n",
    "    best_loss = float('inf')\n",
    "     \n",
    "    for epoch in tqdm(range(n_epochs)):\n",
    "        epoch_loss = 0.0\n",
    "        for batch in tqdm(dataloader, leave=False, desc=f\"Epoch {epoch + 1}/{n_epochs}\", colour=\"#005500\"):\n",
    "            x = batch[0].to(device)\n",
    "\n",
    "            batch_size = len(x)\n",
    "\n",
    "            t = torch.randint(0, ddpm.n_steps, (batch_size,)).to(device)\n",
    "            eps = torch.randn_like(x).to(device)\n",
    "\n",
    "            noise = ddpm(x, t, eps)\n",
    "            noise_est = ddpm.reverse(noise, t)\n",
    "\n",
    "            loss = loss_function(noise, noise_est)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "\n",
    "            epoch_loss += loss.item() * batch_size / len(dataloader.dataset)\n",
    "        \n",
    "        log_string = f\"Loss at epoch {epoch + 1}: {epoch_loss:.3f}\"\n",
    "\n",
    "        if best_loss > epoch_loss:\n",
    "            best_loss = epoch_loss\n",
    "            torch.save(ddpm.state_dict(), store_path)\n",
    "            log_string += \" --> Best model ever (stored)\"\n",
    "\n",
    "        print(log_string)\n",
    "\n",
    "optimizer = torch.optim.Adam(ddpm.parameters(), lr=0.001)     \n",
    "training(ddpm, dataset, n_epochs=1, optimizer=optimizer)        "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "store_path = 'ddpm.pt'\n",
    "\n",
    "# Loading the trained model\n",
    "best_model = CustomDiffusionModel(CustomUnet(), n_steps=n_steps, device=device)\n",
    "best_model.load_state_dict(torch.load(store_path, map_location=device))\n",
    "best_model.eval()\n",
    "print(\"Model loaded\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Diffusion_project.utils import show_images\n",
    "import imageio\n",
    "import einops\n",
    "\n",
    "\n",
    "def generate_new_images(ddpm, n_samples=16, device=None, frames_per_gif=100, gif_name=\"sampling.gif\", c=1, h=28, w=28):\n",
    "    \"\"\"Given a DDPM model, a number of samples to be generated and a device, returns some newly generated samples\"\"\"\n",
    "    frame_idxs = np.linspace(0, ddpm.n_steps, frames_per_gif).astype(np.uint)\n",
    "    frames = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        if device is None:\n",
    "            device = ddpm.device\n",
    "\n",
    "        # Starting from random noise\n",
    "        x = torch.randn(n_samples, c, h, w).to(device)\n",
    "\n",
    "        for idx, t in enumerate(list(range(ddpm.n_steps))[::-1]):\n",
    "            # Estimating noise to be removed\n",
    "            time_tensor = (torch.ones(n_samples, 1) * t).to(device).long()\n",
    "            eta_theta = ddpm.backward(x, time_tensor)\n",
    "\n",
    "            alpha_t = ddpm.alphas[t]\n",
    "            alpha_t_bar = ddpm.alpha_bars[t]\n",
    "\n",
    "            # Partially denoising the image\n",
    "            x = (1 / alpha_t.sqrt()) * (x - (1 - alpha_t) / (1 - alpha_t_bar).sqrt() * eta_theta)\n",
    "\n",
    "            if t > 0:\n",
    "                z = torch.randn(n_samples, c, h, w).to(device)\n",
    "\n",
    "                # Option 1: sigma_t squared = beta_t\n",
    "                beta_t = ddpm.betas[t]\n",
    "                sigma_t = beta_t.sqrt()\n",
    "\n",
    "                # Option 2: sigma_t squared = beta_tilda_t\n",
    "                # prev_alpha_t_bar = ddpm.alpha_bars[t-1] if t > 0 else ddpm.alphas[0]\n",
    "                # beta_tilda_t = ((1 - prev_alpha_t_bar)/(1 - alpha_t_bar)) * beta_t\n",
    "                # sigma_t = beta_tilda_t.sqrt()\n",
    "\n",
    "                # Adding some more noise like in Langevin Dynamics fashion\n",
    "                x = x + sigma_t * z\n",
    "\n",
    "            # Adding frames to the GIF\n",
    "            if idx in frame_idxs or t == 0:\n",
    "                # Putting digits in range [0, 255]\n",
    "                normalized = x.clone()\n",
    "                for i in range(len(normalized)):\n",
    "                    normalized[i] -= torch.min(normalized[i])\n",
    "                    normalized[i] *= 255 / torch.max(normalized[i])\n",
    "\n",
    "                # Reshaping batch (n, c, h, w) to be a (as much as it gets) square frame\n",
    "                frame = einops.rearrange(normalized, \"(b1 b2) c h w -> (b1 h) (b2 w) c\", b1=int(n_samples ** 0.5))\n",
    "                frame = frame.cpu().numpy().astype(np.uint8)\n",
    "\n",
    "                # Rendering frame\n",
    "                frames.append(frame)\n",
    "\n",
    "    # Storing the gif\n",
    "    with imageio.get_writer(gif_name, mode=\"I\") as writer:\n",
    "        for idx, frame in enumerate(frames):\n",
    "            writer.append_data(frame)\n",
    "            if idx == len(frames) - 1:\n",
    "                for _ in range(frames_per_gif // 3):\n",
    "                    writer.append_data(frames[-1])\n",
    "    return x\n",
    "\n",
    "print(\"Generating new images\")\n",
    "generated = generate_new_images(\n",
    "        best_model,\n",
    "        n_samples=100,\n",
    "        device=device,\n",
    "        gif_name=\"fashion.gif\"\n",
    "    )\n",
    "show_images(generated, \"Final result\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
